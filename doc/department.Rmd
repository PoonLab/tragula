---
title: "Departmental analysis"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.width=5, fig.height=5, dpi=150, out.width="50%"
  )
knitr::opts_knit$set(root.dir="~/git/tragula")
```

This document provides a workflow example for retrieving, processing and visualizing the similarity network among faculty members of the Department of Pathology and Laboratory Medicine (PaLM) at my home institution.

## Download text

We will use a Python script to retrieve publication records from the NCBI PubMed database, do some initial text processing, and write the results to JSON files (one per author) into a directory `data/palm`:
```bash
% python scripts/batch_fetch.py --retmax 200 --email your.email@smtp.org data/palm.tsv data/palm
```

Of course you need to replace `your.email@smtp.org` with your own e-mail address.  This is required to query the NCBI database.  In addition, I set up this script to pause for one second between API queries to avoid hitting the NCBI servers with too many requests.  As a result, the script will take about a minute to run to completion.

## Extract word counts

The next step is to process the documents that we've just downloaded into three outputs:
1. a JSON file containing associative arrays of word frequencies for each author;
2. a CSV file containing the term-document co-occurrence matrix for every pair of words that appears at least once in a given document, and;
3. a CSV file linking the integer indices used in the co-occurrence matrix to the actual word, in descending order of the word's overall frequency in the entire corpus.

```bash
% python scripts/analyze.py data/palm --counts results/palm_author.json --matrix results/palm_cooccur.csv --index results/palm_index.csv
```


## Import data

The `index` CSV file generated by `analyze.py` is a list of all words encountered in the text corpus, in descending order by frequency.  The `wordcloud` package provides a nice way of visualizing the most frequent words.
```{r cache=TRUE}
index <- read.csv("results/palm_index.csv")
require(wordcloud, quietly=TRUE)
set.seed(3)
wordcloud(index$word[1:50], index$count[1:50], 
          colors=hcl.colors(20, 'Berlin'), random.color=TRUE)
```


## Word embedding

The term-document co-occurrence matrix that we calculated in the previous step tracks the number of times every word appears in each document.  This matrix becomes very large with even a modest number of documents, so we store the results in a "sparse" format in which we only record the number of times that word $i$ appears in document $j$, skipping all cases in which this count is zero.

We assume that words tending to appear in the same documents fall under a similar topic.  For example, "phylogeny" and "sequence" tend to co-occur in the same abstracts.  The next step calculates the distance between every pair of words based on their co-occurrence in documents.  

To limit the amount of memory required to generate this distance matrix, we limit our analysis to the 5,000 most frequent words.  The frequency distribution of words is very skewed, and words that have a very low frequency are not very useful to comparing authors.  To illustrate this skewed distribution, let's generate a quick barplot:
```{r}
source("scripts/topicspace.R")
tab <- table(index$count)
par(mar=c(5,5,1,1))
plot(x=as.integer(names(tab)), y=as.integer(tab), las=2, 
     pch=19, cex=0.5,
     xlab="Number of times the word appears", ylab="Frequency")
```

For example, there are 4,415 times that a word appears only twice in the entire set of documents (upper-left point).
Note that words that appeared only once were already filtered out during our processing steps).

The `topicspace` function combines the three outputs generated by the Python scripts, and uses a non-linear dimensionality reduction method ([UMAP](https://umap-learn.readthedocs.io/en/latest/)) to "embed" the words into a smaller number of dimensions (controlled by the `n.comp` argument).
This function outputs a custom (`topicspace`) S3 class object for which I've written a few generic methods (`print`, `summary` and `plot`).

```{r cache=TRUE, message=FALSE}
ts <- topicspace(
  index.path = "results/palm_index.csv",
  cooccur.path = "results/palm_cooccur.csv",
  author.path = "results/palm_author.json",
  max.words=5000, n.comp=3)
ts  # calls generic print function
```

Calling `plot` on a `topicspace` object displays the embedding of words into the first two dimensions of the reduced space:
```{r fig.width=6, fig.height=6, res=150, out.width="100%"}
plot(ts)
```

By default the plot function only displays the 100 most frequent words, replacing all other words with a small dot.
Displaying all 5,000 words on this plot would make it impossible to read!
The same function can also be used to resize points in proportion to a specific author's word frequency distribution, and compare it to the distributions of other authors:
```{r fig.width=10, fig.height=6, res=150, out.width="100%"}
par(mfrow=c(1,2))
plot(ts, author="Poon_Art", main="Poon")
plot(ts, author="Garcia_Bertha", main="Garcia")
```


## Earth mover's distance

The final step of the analysis is to calculate a matrix of distances between members of the department, based on the differences in their respective word frequency distributions.
When we map a person's word frequency distribution onto the "topic space" in which we have embedded all the words, we sc

Having embedded words into a "topic space", we are able to compare these distributions even when people do not use exactly the same words.
This is possible because the Wasserstein distance, also known as the Kantorovich-Rubenstein or earth mover's distance, determines the minimum amount of probability mass that needs to be transferred from one point to another 



