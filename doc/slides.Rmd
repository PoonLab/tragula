---
title: "Generating networks of shared research expertise"
author: "Art Poon"
date: 'Departments of Pathology and Laboratory Medicine; Microbiology and Immunology; Computer Science'
output: 
  revealjs::revealjs_presentation:
    self_contained: true
    css: Rmd-style.css
    center: true
    highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir="~/git/tragula")
```

```{r echo=FALSE}
# load faculty information
files <- c("data/acb.tsv", "data/biochem.tsv", "data/epibio.tsv", 
           "data/mbp.tsv", "data/mimm.tsv", "data/palm.tsv", 
           "data/ppharm.tsv")
fac <- do.call(rbind, lapply(files, function(f) {
  dept <- gsub("\\.tsv", "", basename(f))
  tsv <- read.csv(f, sep='\t')
  tsv <- tsv[,1:4]
  names(tsv) <- c("lastname", "forename", "initials", "query")
  tsv$affil <- dept
  tsv
  }))
# make some more convenient labels
fac$labels <- paste(substr(fac$forename, 1, 1),
                    substr(fac$lastname, 1, 4), sep="")
  #paste(fac$initials, substr(fac$lastname, 2, 2), sep="")
fac$filename <- gsub(" ", "_", paste(fac$lastname, fac$forename, sep="_"))
```

## Expertise mapping

* A common task for academic units is to identify clusters of research interests/expertise
  * Recruiting prospective graduate students
  * Facilitating interactions among faculty
* This task is often done manually, which is subjective, tedious, and time-consuming.

---

## Objectives

* Develop an automated workflow to:
  * extract research themes from each member's contributions to the peer-reviewed literature
  * use multivariate analysis to determine each member's relation to their  peers based on shared research themes
  * develop visualizations of analysis outputs
* Release as an open source project that might be used by other groups

---

## Database

* NCBI PubMed can be accessed through the Entrez API
  * Results are returned in an XML format by default
* Biopython provides an interface to this API and XML parsing functions

```python
>>> from Bio import Entrez
>>> Entrez.email = "apoon42@uwo.ca"
>>> response = Entrez.esearch(db="pubmed", term="Art Poon")
>>> result = Entrez.read(response)
>>> result
{'Count': '112', 'RetMax': '20', 'RetStart': '0', 'IdList': ['38485563', '38355869', '37547379', '37463439', '37292785', '37187604', '37101658', '36846827', '36809686', '36632480', '36449514', '36395093', '36178092', '35916373', '37552744', '35500121', '35202429', '34941890', '37124703', '34578305'], 'TranslationSet': [{'From': 'Art Poon', 'To': 'Poon, Art[Full Author Name]'}], 'QueryTranslation': 'poon, art[Author]'}
```

---

## Data collection

* Retrieve title, keywords and abstract for all PubMed records associated with author query
  * Exclude titles starting with "Author Correction" or "Correction"
* Manually compose query to exclude other authors
  * *e.g.*, `Cameron, Lisa NOT Cameron, LA NOT Cameron LJ`
* Write results to a JSON file for each member.

---

## Natural language processing

* Using Python module `nltk` (natural language toolkit)
* Word tokenizer breaks strings down into words and punctuation.
* Part-of-speech tagging categorizes words by their meaning and context.
* Exclude punctuation, coordinating conjunctions and other words in a manually-curated list, *e.g.*, "when"

---

```{r echo=FALSE, message=FALSE, cache=TRUE}
index <- read.csv("results/b7_index.csv")
require(wordcloud, quietly=TRUE)
set.seed(3)
wordcloud(index$word[1:200], index$count[1:200], 
          colors=hcl.colors(20, 'Berlin'), random.color=TRUE)
```
<center>
200 most frequent words in corpus of all B7 faculty
</center>


---

## Term-document matrix

* Count the co-occurrences of words per document (title+keywords+abstract)
  * 51,103 words from 16,895 documents

```{r echo=FALSE, cache=TRUE}
index <- read.csv("results/b7_index.csv")
sparse <- read.csv("results/b7_cooccur.csv", header=F)
names(sparse) <- c('doc.idx', 'word.idx', 'count')

top5 <- sparse[sparse$word.idx < 5, ]  # most frequent words
totals <- sapply(split(top5$count, top5$doc.idx), sum)
top10 <- unique(top5$doc.idx)[order(totals, decreasing = TRUE)[1:10]]

temp <- sparse[sparse$word.idx < 5 & sparse$doc.idx %in% top10, ]
temp$doc.idx <- as.integer(as.factor(temp$doc.idx))  # renumber
tdm <- matrix(0, nrow=5, ncol=10)
rownames(tdm) <- index$word[1:5]
colnames(tdm) <- paste("D", 1:10, sep="")
for (row in 1:nrow(temp)) {
  tdm[temp$word.idx[row]+1, temp$doc.idx[row]] <- temp$count[row]
}
knitr::kable(tdm, "html")
```

---

```{r echo=FALSE, cache=TRUE, message=FALSE}
source("scripts/topicspace.R")
ts <- topicspace(
  index.path = "results/b7_index.csv",
  cooccur.path = "results/b7_cooccur.csv",
  author.path = "results/b7_counts.json",
  max.words=10000, n.comp=3)
```

```{r message=FALSE}
set.seed(2)
plot(ts, text.cex=0.7, idx=sample(1:1e4, 100))  # calls plot.topicspace()
```
<center>
We calculate the cosine distance for every pair of words and then apply UMAP to the distance matrix to project words into a "topicspace".
</center>

---

```{r fig.width=10, fig.height=5, out.width="100%"}
par(mfrow=c(1,2))  # side-by-side format
plot(ts, author=39)  # plot(ts, author="Poon_Art")
plot(ts, author=77)
```
<center>
We can then map the word frequency distribution for each author's corpus to this topic space to produce a "fingerprint".
</center>

---

## Wasserstein metric

* Each fingerprint is a discrete probability distribution.
  * The Wasserstein metric or Earth mover's distance quantifies the smallest amount of work needed to reshape one distribution to another.
* Comparing all pairs of fingerprints yields a distance matrix of authors.
* There are numerous ways of visualizing this matrix --- I favoured a k-nearest neighbours graph.

---

```{r cache=TRUE}
# this takes a long time to compute
load("results/b7_wdist_10E4.RData")
```

```{r message=FALSE, warning=FALSE, fig.width=10, fig.height=7}
source("scripts/visualize.R")
idx <- match(names(ts$by.author), fac$filename)
labels <- fac$label[idx]
groups <- fac$affil[idx]
g <- make.knn(wdist, k=3)

require(r2d3, quietly = TRUE)
r2d3(data=export.json(g, labels, groups), script="js/graph.js", d3_version="4")
```

---

## Further work

* Clustering analysis, *i.e.*, community detection
  * Identify research themes by mapping back to topicspace.
* Build an interactive web animation
* Finish writing this up and submit a manuscript.
* Finish collecting data for all of Schulich (clinical departments are big).
